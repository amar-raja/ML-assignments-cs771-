{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "525324e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f177a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GIVEN GD FUNCTION\n",
    "def gradient_descent(gradient,init_,learn_rate,n_iter=50,tol=1e-06):\n",
    "    x=init_  #input\n",
    "    for i in range(n_iter):\n",
    "        delta = -learn_rate*gradient(x)\n",
    "        if np.all(np.abs(delta)<=tol):\n",
    "            break\n",
    "        x+= delta\n",
    "    return round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ca811",
   "metadata": {},
   "source": [
    "## 1(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194721ef",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}\n",
    "(i)~f(x)=x^{2}+3 x+4 \\\\\n",
    "~~~~~f^{\\prime}(x)=2 x+3 \\\\\n",
    "(ii)~g(x)=x^{4}-3 x^{2}+2 x \\\\\n",
    "~~~~~~g^{\\prime}(x)=4 x^{3}-6 x+2\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "717398d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num =  0.01        #increment value after each iteration for\n",
    "slope1 = []          # collecting all slopes returned by gradient\n",
    "initial_val = [-2,-1,0,1,2]    # my range fo inital values[-2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5309bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in initial_val:\n",
    "    lr_rate = 0.1\n",
    "    for i in range(10):\n",
    "        t = gradient_descent(lambda v: 2 * v + 3,j,lr_rate)\n",
    "        slope1.append(t)\n",
    "        lr_rate = num+0.01     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1ee8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMA for f(x) is : -1.565\n"
     ]
    }
   ],
   "source": [
    "print('MINIMA for f(x) is :',min(slope1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b52a62b",
   "metadata": {},
   "source": [
    "1(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3fdbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num =  0.01        #increment value after each iteration for\n",
    "slope2 = []          # collecting all slopes returned by gradient\n",
    "initial_val = [-2,-1,0,1,2]    # my range fo inital values[-2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a4067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in initial_val:\n",
    "    lr_rate = 0.1\n",
    "    for i in range(10):                  #performing with 10 learning rates\n",
    "        t = gradient_descent(lambda v: 4 * (v**3) - (6*v) + 2,j,lr_rate)\n",
    "        slope2.append(t)\n",
    "        lr_rate = num+0.01         #updating learningrate   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60125dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#g(x) has two minimas\n",
    "minima= set(slope2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e106b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MINIMA for g(x) is : {1.0, -1.366}\n"
     ]
    }
   ],
   "source": [
    "print('MINIMA for g(x) is :',minima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8867d91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22694980",
   "metadata": {},
   "source": [
    "## 1b) Gradient function for LINEAR REGRESSION  =>  y = mx+b ,  x belongs to $R^{D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072223d",
   "metadata": {},
   "source": [
    "MEAN SQUARED ERROR LOSS FUNCTION\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L(s) &=\\frac{1}{n}\\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\\\\n",
    "&=\\frac{1}{n}\\sum_{i=1}^{N}\\left[y_{i}-\\left(m x_{i}+c\\right)\\right]^{2}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c73d4a",
   "metadata": {},
   "source": [
    "DERIVATIVES OF SLOPE AND INTERCEPT FORM\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial(s)}{\\partial(m)}=\\frac{2}{n} \\cdot \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right) \\cdot\\left(-x_{i}\\right)$\n",
    "\\\\\n",
    "\\frac{\\partial(s)}{\\partial(b)}=\\frac{-2}{n} \\cdot \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)$\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85e7642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient func that can work for multivaraible linear regression also\n",
    "def gradient(x,y,m_curr,b_curr):\n",
    "    y_predict = np.dot(x,m_curr) + b_curr\n",
    "    inter_dr = -2 * np.mean(y-y_predict)\n",
    "    coeff_dr = (-2)* np.dot(y-y_predict,x)\n",
    "    g = [inter_dr,coeff_dr]\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2634ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35070f54",
   "metadata": {},
   "source": [
    "## 1c) GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a26dc014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#INPUT DATA\n",
    "np.random.seed(0)\n",
    "x=2.5 * np.random.rand(10000)+1.5\n",
    "res = 1.5 *np.random.rand(10000)\n",
    "y = 2+0.3 * x +res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab1e1048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADIENT FUNCTION \n",
    "def gradient1gd(x,y,m_curr,b_curr):\n",
    "    n= x.shape[0]         #number of rows\n",
    "\n",
    "    y_predict = np.dot(x,m_curr) + b_curr    #PREDICTED Y VALUE\n",
    "    \n",
    "    inter_dr = -2 * np.mean(y-y_predict)                #INTERCEPT derivative\n",
    "    coeff_dr = ((-2/n)* np.dot(y-y_predict,x))          #COEFFICIENTS derivative\n",
    "    return np.array([coeff_dr,inter_dr])        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "520e252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(gradient1,init_,learn_rate,X_train,y_train,n_iter=1000,tol=1e-06):\n",
    "    x  =init_               #coefficent,Intercepts are stored\n",
    "    for i in range(n_iter):\n",
    "        g = gradient1(X_train,y_train,x[0],x[1])      #calculating GRADIENT\n",
    "        delta = -(learn_rate * g)                   #STEPSIZE\n",
    "        if (np.all(np.abs(delta[1])<=tol)) & (np.all(np.abs(delta[0])<=tol)):   #IF BOTH allcoeff and intercept are less than 10^-6\n",
    "            print('breaked at iteration num--',i)\n",
    "            break\n",
    "        x[0]+=delta[0]           #UPDATING CEOFFICIENTS\n",
    "        x[1]+=delta[1]           #UPDATING INTEERCEPT\n",
    "    res = [np.round(i*1000)/1000 for i in x]     #RETURING COEFF,INTERCEPT\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5028a62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaked at iteration num-- 881\n",
      "INTERCEPT: 0.304 , SLOPE: 2.732\n",
      "time taken is:  0.10027909278869629\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "r1 =gradient_descent(gradient1gd,[1,0],0.1,x,y)    #(2,0.3) here w,b\n",
    "print('INTERCEPT: {} , SLOPE: {}'.format(r1[0],r1[1]))\n",
    "print('time taken is: ',time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc97520",
   "metadata": {},
   "source": [
    "CALCULATING FOR DIFFERENT LEARNING RATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2508224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERCEPT: 0.555 , SLOPE: 2.001 ,with l_rate = 0.01,time: 0.11586499214172363\n",
      "INTERCEPT: 0.328 , SLOPE: 2.662 ,with l_rate = 0.03,time: 0.131483793258667\n",
      "INTERCEPT: 0.306 , SLOPE: 2.726 ,with l_rate = 0.05,time: 0.08461928367614746\n",
      "INTERCEPT: 0.304 , SLOPE: 2.732 ,with l_rate = 0.07,time: 0.10024094581604004\n",
      "breaked at iteration num-- 871\n",
      "INTERCEPT: 0.304 , SLOPE: 2.732 ,with l_rate = 0.1,time: 0.06899499893188477\n"
     ]
    }
   ],
   "source": [
    "lr_rate = [0.01,0.03,0.05,0.07,0.1]\n",
    "for i in range(len(lr_rate)):\n",
    "    start_time = time.time()\n",
    "    r1 =gradient_descent(gradient1gd,[0,0],lr_rate[i],x,y)\n",
    "    t = time.time()-start_time\n",
    "    print('INTERCEPT: {} , SLOPE: {} ,with l_rate = {},time: {}'.format(r1[0],r1[1],lr_rate[i],t))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9f63be",
   "metadata": {},
   "source": [
    "THIS DATASET HAS OPTIMAL LEARNING RATES = [0.07,0.1] for GRADIENT DESCENT FOR 1000 iterations \n",
    "(around 870th iteration it breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d806f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ea3c799",
   "metadata": {},
   "source": [
    "## 1D) MINIBATCH  GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6647928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient3(x,y,m_curr,b_curr):\n",
    "    n= x.shape[0]    #NUMBER OF ROWS\n",
    "    \n",
    "    y_predict = np.dot(x,m_curr) + b_curr     #PREDICTED Y VALUE\n",
    "    \n",
    "    inter_dr = -2 * np.mean(y-y_predict)                         #INTERCEPT derivative\n",
    "    coeff_dr = ((-2)* np.dot(y-y_predict,x))/n                   #COEFFICIENTS derivative\n",
    "    return np.array([coeff_dr,inter_dr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78d764ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MBgradient_descent(gradient,init_,learn_rate,X_train,y_train,batch_size,n_iter=1000,tol=1e-06):\n",
    "    x  =init_\n",
    "    inner_break = 0\n",
    "    for i in range(n_iter):\n",
    "        idx = random.sample(range(X_train.shape[0]),batch_size)       #SELECTING THE INDEX's OF A BATCH RANDOMLY\n",
    "        g = gradient(X_train[idx],y_train[idx],x[0],x[1])                      #calculating GRADIENT by sending a batch\n",
    "        delta = -(learn_rate * g)                                             #STEPSIZE\n",
    "        if (np.all(np.abs(delta[1])<=tol)) & (np.all(np.abs(delta[0])<=tol)):\n",
    "            break\n",
    "        x[0]+=delta[0]                    #UPDATING CEOFFICIENTS\n",
    "        x[1]+=delta[1]                     #UPDATING INTERCEPT\n",
    "        \n",
    "    res = [np.round(i*1000)/1000 for i in x]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d29edf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.308, 2.73]\n",
      "time taken is:  0.7197997570037842\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(MBgradient_descent(gradient3,[1,0],0.1,x,y,800))\n",
    "print('time taken is: ',time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a345645",
   "metadata": {},
   "source": [
    "CALCULATING FOR DIFFERENT LEARNING RATES AND DIFFERENT BATCHSIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e604822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERCEPT: 0.294 , SLOPE: 2.736  with l_rate: 0.07, batchsize: 100\n",
      "INTERCEPT: 0.263 , SLOPE: 2.699  with l_rate: 0.1, batchsize: 100\n",
      "INTERCEPT: 0.336 , SLOPE: 2.742  with l_rate: 0.07, batchsize: 200\n",
      "INTERCEPT: 0.288 , SLOPE: 2.733  with l_rate: 0.1, batchsize: 200\n",
      "INTERCEPT: 0.316 , SLOPE: 2.732  with l_rate: 0.07, batchsize: 300\n",
      "INTERCEPT: 0.3 , SLOPE: 2.73  with l_rate: 0.1, batchsize: 300\n",
      "INTERCEPT: 0.319 , SLOPE: 2.739  with l_rate: 0.07, batchsize: 400\n",
      "INTERCEPT: 0.307 , SLOPE: 2.73  with l_rate: 0.1, batchsize: 400\n",
      "INTERCEPT: 0.293 , SLOPE: 2.73  with l_rate: 0.07, batchsize: 500\n",
      "INTERCEPT: 0.303 , SLOPE: 2.732  with l_rate: 0.1, batchsize: 500\n",
      "INTERCEPT: 0.294 , SLOPE: 2.726  with l_rate: 0.07, batchsize: 600\n",
      "INTERCEPT: 0.316 , SLOPE: 2.738  with l_rate: 0.1, batchsize: 600\n",
      "INTERCEPT: 0.31 , SLOPE: 2.73  with l_rate: 0.07, batchsize: 700\n",
      "INTERCEPT: 0.306 , SLOPE: 2.735  with l_rate: 0.1, batchsize: 700\n",
      "INTERCEPT: 0.299 , SLOPE: 2.734  with l_rate: 0.07, batchsize: 800\n",
      "INTERCEPT: 0.292 , SLOPE: 2.73  with l_rate: 0.1, batchsize: 800\n",
      "INTERCEPT: 0.304 , SLOPE: 2.727  with l_rate: 0.07, batchsize: 900\n",
      "INTERCEPT: 0.304 , SLOPE: 2.73  with l_rate: 0.1, batchsize: 900\n",
      "INTERCEPT: 0.309 , SLOPE: 2.735  with l_rate: 0.07, batchsize: 1000\n",
      "INTERCEPT: 0.318 , SLOPE: 2.733  with l_rate: 0.1, batchsize: 1000\n"
     ]
    }
   ],
   "source": [
    "lr_rate = [0.07,0.1]\n",
    "batchsize= [100,200,300,400,500,600,700,800,900,1000]\n",
    "for j in range(len(batchsize)):\n",
    "    for i in range(len(lr_rate)):\n",
    "        r1 =MBgradient_descent(gradient3,[1,0],lr_rate[i],x,y,batchsize[j])\n",
    "        print('INTERCEPT: {} , SLOPE: {}  with l_rate: {}, batchsize: {}'.format(r1[0],r1[1],lr_rate[i],batchsize[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1faf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb4cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de0ecc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientsc(x,y,m_curr,b_curr):    \n",
    "    y_predict = np.dot(x,m_curr) + b_curr\n",
    "    \n",
    "    inter_dr = -2 * (y-y_predict)\n",
    "    coeff_dr = ((-2)* np.dot(y-y_predict,x))\n",
    "    return np.array([coeff_dr,inter_dr])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcace2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(gradient,init_,learn_rate,X_train,y_train,n_iter=10000,tol=1e-06):\n",
    "    x  =init_\n",
    "    inner_break = 0\n",
    "    for i in range(n_iter):\n",
    "        idx = np.random.randint(0,X_train.shape[0]-1)             #SELECTING THE INDEX OF DATAPOINT RANDOMLY\n",
    "        g = gradient(X_train[idx],y_train[idx],x[0],x[1])\n",
    "        delta = -(learn_rate * g)                                #STEPSIZE\n",
    "\n",
    "        if (np.all(np.abs(delta[1])<=tol)) & (np.all(np.abs(delta[0])<=tol)):\n",
    "            print('breaked here at iteration--',i)\n",
    "            inner_break =1\n",
    "            break\n",
    "        x[0]+=delta[0]                      #UPDATING CEOFFICIENTS\n",
    "        x[1]+=delta[1]                      #UPDATING INTERCEPT\n",
    "    \n",
    "    res = [np.round(i*1000)/1000 for i in x]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7afdfd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaked here at iteration-- 2447\n",
      "[0.398, 2.268]\n",
      "time taken is:  0.07452154159545898\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(stochastic_gradient_descent(gradientsc,[1,0],0.006,x,y))           #0.3,2\n",
    "print('time taken is: ',time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5151253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERCEPT: 0.323 , SLOPE: 2.699, with l_rate = 0.004,timetaken: 0.29438328742980957\n",
      "breaked here at iteration-- 9499\n",
      "INTERCEPT: 0.277 , SLOPE: 2.753, with l_rate = 0.005,timetaken: 0.28528928756713867\n",
      "INTERCEPT: 0.292 , SLOPE: 2.709, with l_rate = 0.006,timetaken: 0.2983286380767822\n",
      "INTERCEPT: 0.216 , SLOPE: 2.716, with l_rate = 0.007,timetaken: 0.29424500465393066\n",
      "breaked here at iteration-- 9427\n",
      "INTERCEPT: 0.299 , SLOPE: 2.758, with l_rate = 0.008,timetaken: 0.2754690647125244\n",
      "INTERCEPT: 0.308 , SLOPE: 2.63, with l_rate = 0.009000000000000001,timetaken: 0.3031482696533203\n",
      "INTERCEPT: 0.27 , SLOPE: 2.706, with l_rate = 0.010000000000000002,timetaken: 0.29170775413513184\n",
      "INTERCEPT: 0.241 , SLOPE: 2.694, with l_rate = 0.011000000000000003,timetaken: 0.29370570182800293\n",
      "INTERCEPT: 0.29 , SLOPE: 2.723, with l_rate = 0.012000000000000004,timetaken: 0.2951226234436035\n",
      "INTERCEPT: 0.258 , SLOPE: 2.707, with l_rate = 0.013000000000000005,timetaken: 0.28639912605285645\n",
      "INTERCEPT: 0.313 , SLOPE: 2.738, with l_rate = 0.014000000000000005,timetaken: 0.29741692543029785\n",
      "INTERCEPT: 0.321 , SLOPE: 2.717, with l_rate = 0.015000000000000006,timetaken: 0.28558802604675293\n",
      "INTERCEPT: 0.237 , SLOPE: 2.672, with l_rate = 0.016000000000000007,timetaken: 0.29563188552856445\n",
      "INTERCEPT: 0.383 , SLOPE: 2.657, with l_rate = 0.017000000000000008,timetaken: 0.2941131591796875\n",
      "INTERCEPT: 0.362 , SLOPE: 2.725, with l_rate = 0.01800000000000001,timetaken: 0.2830162048339844\n"
     ]
    }
   ],
   "source": [
    "lr_rate = 0.004\n",
    "for i in range(15):\n",
    "    start_time = time.time()\n",
    "    r1 =stochastic_gradient_descent(gradientsc,[1,0],lr_rate,x,y)\n",
    "    timetaken = time.time() - start_time\n",
    "    print('INTERCEPT: {} , SLOPE: {}, with l_rate = {},timetaken: {}'.format(r1[0],r1[1],lr_rate,timetaken))\n",
    "    lr_rate += 0.001\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628c98f",
   "metadata": {},
   "source": [
    "FROM THE ABOVE WE CAN SAY THAT SGD CONVERGES FASTER TO OPTIMAL VALUE THAN GRADIENTDESCENT and \n",
    "MINI BATCH GRADIENTDESCENT.,BUT AT THE COMPROMISE OF PREECISION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd63890",
   "metadata": {},
   "source": [
    "## performance comparision "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5513a9",
   "metadata": {},
   "source": [
    "OPTIMAL GRADIENT DESCENT FOR 1000 epochs   (reuires around 900) for this data  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9dd570",
   "metadata": {},
   "source": [
    "OPTIMAL LR_RATES(0.07,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "913767d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaked at iteration num-- 871\n"
     ]
    }
   ],
   "source": [
    "gd=[0,0]\n",
    "timegd=[0,0]\n",
    "#CALCULATING TIMES\n",
    "start_time = time.time()\n",
    "gd[0] = gradient_descent(gradient1gd,[0,0],0.07,x,y)\n",
    "timegd[0] = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "gd[1] = gradient_descent(gradient1gd,[0,0],0.1,x,y)\n",
    "timegd[1] = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb9cfd",
   "metadata": {},
   "source": [
    "#OPTIMAL STOCHASTIC GRADIENT DESCENT  REQUIRES  (i.e. around 8000 iterations only) and sometimes even lessthan 1000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7cced0",
   "metadata": {},
   "source": [
    "#OPTIMAL LR_RATES are randomized and unpredicatble but for this data (0.005,0.01) are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9065026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "breaked here at iteration-- 3872\n"
     ]
    }
   ],
   "source": [
    "sgd = [0,0]\n",
    "timesc=[0,0]\n",
    "#CALCULATING TIMES\n",
    "start_time = time.time()\n",
    "sgd[0] = stochastic_gradient_descent(gradientsc,[1,0],0.005,x,y)\n",
    "timesc[0] = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "sgd[1] = stochastic_gradient_descent(gradientsc,[1,0],0.01,x,y,n_iter=1000)\n",
    "timesc[1] = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b03e4dbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.11587190628051758, 0.10028791427612305],\n",
       " [0.19533634185791016, 0.032259464263916016])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timegd,timesc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05419bb8",
   "metadata": {},
   "source": [
    "CONCLUSION-- STOCASTIC CONVERGES FASTER TO OPTIMAL BUT DUE TO RANDOMNESS IT IS NOT ALWAYS GURANTEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336c334",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4a5ec6e",
   "metadata": {},
   "source": [
    "##  1E)OPTIMAL BATCH SIZE FOR MINIBATCH GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35107c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR 1000 ITERATIONS\n",
    "batch_sizes_mgd = [100,200,300,400,500,600,700,800,900,1000,5000]\n",
    "\n",
    "lr_rate_mgd = [0.07,0.1]        #WITH OPTIMAL RATES OF MGD FOR THIS DATA\n",
    "timemgd = []\n",
    "\n",
    "for i in range(len(batch_sizes_mgd)):\n",
    "    for j in range(len(lr_rate_mgd)):\n",
    "        start_time = time.time()\n",
    "        MBgradient_descent(gradient3,[1,0],lr_rate_mgd[j],x,y,batch_sizes_mgd[i],n_iter=1000)\n",
    "        timemgd.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4661a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5abc90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "least_time = min(timemgd)\n",
    "\n",
    "ind = timemgd.index(least_time)\n",
    "size_of_listbatch = len(batch_sizes_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "870c4cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMAL BATCH SIZE FOR THIS DATA >> 100\n"
     ]
    }
   ],
   "source": [
    "print('OPTIMAL BATCH SIZE FOR THIS DATA >>',batch_sizes_mgd[math.floor(ind/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7fa96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR 100 ITERATIONS\n",
    "timemgd = []\n",
    "\n",
    "for i in range(len(batch_sizes_mgd)):\n",
    "    for j in range(len(lr_rate_mgd)):\n",
    "        start_time = time.time()\n",
    "        MBgradient_descent(gradient3,[1,0],lr_rate_mgd[j],x,y,batch_sizes_mgd[i],n_iter=100)\n",
    "        timemgd.append(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "280815a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "least_time = min(timemgd)\n",
    "\n",
    "ind = timemgd.index(least_time)\n",
    "size_of_listbatch = len(batch_sizes_mgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb5a303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMAL BATCH SIZE FOR THIS DATA >> 200\n"
     ]
    }
   ],
   "source": [
    "print('OPTIMAL BATCH SIZE FOR THIS DATA >>',batch_sizes_mgd[math.floor(ind/2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94a5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b272c84",
   "metadata": {},
   "source": [
    "CONCLUSION--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb978d7",
   "metadata": {},
   "source": [
    "-MINIBATCH GRADIENT DESCENT IS ALSO ASSOCIATED WITH RANDOMNESS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11152944",
   "metadata": {},
   "source": [
    "-- PERFORMANCE OF MGD DEPENDS ON BATCH SIZE AND NUMBER_OF_ iTERATIONS SIMULATANEOULSY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096a8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "334c47c2",
   "metadata": {},
   "source": [
    "# Q2. Bayesian Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b84b8c",
   "metadata": {},
   "source": [
    "![](Bayesian_Network.jpg)\n",
    "\n",
    "(i) The probability that someone has both cold and a fever\n",
    "\n",
    "\\begin{aligned}\n",
    "&P(\\text {Cold and Fever})\\\\\n",
    "&=P(C \\cap F)\\\\\n",
    "&=P(F \\mid C) P(C)\\\\\n",
    "&=0.307\\times 0.02\\\\\n",
    "&=0.00614\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05b746d",
   "metadata": {},
   "source": [
    "(ii) The  probability that  someone  who has  a  cough has  a  cold.\n",
    "\n",
    "\\begin{aligned}\n",
    "&P(\\text {Cold} \\mid \\text {Cough})=\\frac{P(\\text {Cold and Cough})}{P(\\operatorname{Cough})}\\\\\n",
    "&P({Cough})=P(\\text {Lung}) P\\left(Cold) P(\\text {Cough} \\mid \\text {Lung and Cold})+\\right.P(\\neg Lung) P(\\text {Cold}) P(Cough \\mid {\\neg Lung \\space and \\space Cold)}+P(\\neg Cold) P(\\text {Lung}) P(Cough \\mid {\\neg Cold\\space and \\space Lung)}+P(\\neg Cold) P(\\neg {Lung}) P(Cough \\mid {\\neg Lung \\space and \\space \\neg Cold)}.\\\\\n",
    "&=P(\\text { Lung })\\times 0.02 \\times 0.7525 +\\\\\n",
    "&~~~~~ P{(\\neg Lung) }\\times 0.02 \\times0.505+\\\\\n",
    "&~~~~~ P\\text{(Lung) }\\times 0.98 \\times0.505+\\\\\n",
    "&~~~~~ P{(\\neg Lung) }\\times 0.98 \\times 0.01\\quad-(1)\\\\\n",
    "&P(\\text {Lung})=P(\\text {Lung} \\mid \\text {Smokes)}\\times P(Smokes)+P(\\text {Lung} \\mid {\\neg Smokes)}\\times P(\\neg Smokes)\\\\\n",
    "&~~~~~~~~~~~~~~~=0.1009 \\times 0.2+0.001 \\times 0.8\\\\\n",
    "&~~~~~~~~~~~~~~~=0.02098  \\quad-(2)\\\\\n",
    "&P(\\neg Lung)=P(\\neg Lung \\mid \\text {Smokes}) P(\\text {Smokes})+P { (\\neg Lung) }\\mid {\\neg Smokes) } P({ \\neg Smokes) }\\\\\n",
    "&~~~~~~~~~~~~~~~~=0.8991 \\times 0.2+0.999 \\times 0.8\\\\\n",
    "&~~~~~~~~~~~~~~~~=0.97902\\quad-(3)\\\\\n",
    "&Putting ~ (2) ~ and ~ (3) ~ in ~ (1)\\\\\n",
    "&P(Cough) = 0.02098\\times 0.02 \\times 0.7525 + 0.97902\\times 0.02 \\times0.505+ 0.02098\\times 0.98 \\times0.505+0.97902\\times 0.98 \\times 0.01\\\\\n",
    "&P(Cough)= 0.030181 \\\\ \n",
    "&Now, \\\\\n",
    "&{P(\\text {Cold and Cough})} =P(Cough \\mid Cold) P(Cold) = P(Cough \\mid Cold) \\times 0.02 \\\\\n",
    "&P(Cough \\mid Cold) = P(Cough \\mid Cold,Lung) \\times P(Lung) + P(Cough \\mid Cold, \\neg Lung) \\times P(\\neg Lung)\\\\\n",
    "& = 0.7525\\times0.02098 + 0.505\\times0.97902 \\\\\n",
    "& P(Cough \\mid Cold) = 0.5102\\\\\n",
    "&{P(\\text {Cold and Cough})} = 0.5102 \\times 0.02 = 0.010204 \\\\\n",
    "&Finally, \\\\\n",
    "&Using \\space P(Cold \\space and  \\space Cough) \\space and \\space P(Cough),\\\\\n",
    "&P(\\text {Cold} \\mid \\text {Cough}) = 0.010204 \\div 0.030181 = 0.3381\\\\                                                                                                        \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9053c8d",
   "metadata": {},
   "source": [
    "# Q3. Derive the MLE for the parameters of a k-sided multinomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e2a0ef",
   "metadata": {},
   "source": [
    "Sol: Joint probability of Multinomial distribution is the Likelihood given as $L(P)$\n",
    "$$\n",
    "L(p)=\\frac{n !}{x_{1} ! x_{2} ! x_{3} ! \\ldots x_{k} !} p_{1}^{x_{1}} p_{2}^{x_{2}} p_{3}^{x_{3}} \\ldots p_{k}^{x_{k}}\n",
    "$$\n",
    "Let $\\theta:\\left\\{\\theta_{1}, \\theta_{2}, \\ldots \\theta_{k}\\right\\}$ such that $\\sum_{i=1}^{k} \\theta_{k}=1$ \n",
    "The log likelihood is,\n",
    "$$\n",
    "l(P)=\\ln \\left(\\prod_{i=1}^{k} \\theta_{i}^{N_{i}}\\right) ;  \\begin{gathered}\n",
    "\\space N_{i} \\text { be the number of times face 'i'}\n",
    "\\text { appeared }\n",
    "\\end{gathered}\n",
    "$$\n",
    "Now, \n",
    "$\\space \\theta^{M L E}=\\operatorname{argmax}\\left(\\ln \\prod_{i=1}^{k} \\theta_{i}^{N i}\\right)\\\\$\n",
    "In multinomial last term is not a random variable but a result of all the other random variables,\n",
    "$\\Rightarrow \\theta^{M L E}=\\operatorname{argmax}\\ln \\left[\\left(\\prod_{i=1}^{k-1} \\theta_{i}^{N i}\\right)\\left(1-\\sum_{i=1}^{k-1} \\theta_{i}\\right)^{N_{k}}\\right]$\n",
    "$=\\operatorname{argmax}\\left[\\sum_{i=1}^{k-1}\\right.$  $\\left.N_{i} \\ln \\theta_{i}+N_{k} \\ln \\left(1-\\sum_{i=1}^{k-1} \\theta_{i}\\right)\\right]$\n",
    "$=\\frac{\\partial}{\\partial \\theta_{i}}\\left[\\sum_{i=1}^{k-1} N_{i} \\ln \\theta_{i}+N_{k} \\ln \\left(1-\\sum_{i=1}^{k-1} \\theta_{i}\\right)\\right]$\n",
    "$\\Rightarrow  \\frac{N_{i}}{\\theta_{i}}+\\frac{N_{k}}{1-\\sum_{i=1}^{N-1} \\theta_{i}}(-1)=0$\n",
    "$\\Rightarrow \\theta_{i}=\\frac{N_{i}}{N_{k}} \\theta_{k}$\n",
    "We know that, $\\theta_{1}+\\theta_{2}+\\cdots+\\theta_{k}=1$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\Rightarrow\\left[\\frac{N_{1}}{N_{k}} \\theta_{k}+\\frac{N_{2}}{N_{k}} \\theta_{k}+\\cdots \\frac{N_{k}}{N_{k}} \\theta_{k}\\right]=1 \\\\\n",
    "&\\Rightarrow \\quad \\theta_{k}=\\frac{N_{k}}{\\sum_{i=1}^{k} N_{i}}=\\frac{N_{k}}{N} \\\\\n",
    "&\\therefore \\quad \\theta_{k}=\\frac{N_{k}}{N} ; where \\space N=N_{1}+N_{2}+\\cdots+N_{k}\n",
    "\\end{aligned}\n",
    "$$\n",
    "The probability distribution that maximizes the likelihood of observing the data is $\\quad P=\\left(\\frac{N_{1}}{N}, \\frac{N_{2}}{N},\\cdots,\\frac{N_{k}}{N}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf0196b",
   "metadata": {},
   "source": [
    "#FINISHED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a124eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b18fa90cb66ace53462d50537ff1fb3d73bc0b7d947ec634842325f4f44018c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
